{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code illustrates the fast AI implementation of the unsupervised \"biological\" learning algorithm from [Unsupervised Learning by Competing Hidden Units](https://doi.org/10.1073/pnas.1820458116) on MNIST data set. \n",
    "If you want to learn more about this work you can also check out this [lecture](https://www.youtube.com/watch?v=4lY-oAY0aQU) from MIT's [6.S191 course](http://introtodeeplearning.com/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines paramaters of the algorithm: \n",
    "- `eps0`: initial learning rate that is linearly annealed during training\n",
    "- `N_hid`: number of hidden units that are displayed as an `Ky` by `Kx` array by the `draw_weights` function defined below\n",
    "- `mu`: the mean of the gaussian distribution that initializes the weights\n",
    "- `sigma`: the standard deviation of that gaussian \n",
    "- `Nep`: number of epochs\n",
    "- `N_batch`: size of the minibatch\n",
    "- `prec`: parameter that controls numerical precision of the weight updates\n",
    "- `delta`: the strength of the anti-hebbian learning\n",
    "- `p`: Lebesgue norm of the weights; `k` - ranking parameter. \n",
    "- `N_in`: number of input units\n",
    "- `val_split`: the proportion of validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps0 = 4e-2    # initial learning rate for the unsupervised part\n",
    "Kx = 10\n",
    "Ky = 10\n",
    "hid_disp = Kx*Ky    # number of hidden units that are displayed in Ky by Kx array\n",
    "N_hid = 1000    # number of hidden units\n",
    "mu = 0.0\n",
    "sigma = 1.0\n",
    "Nep = 1000     # number of epochs\n",
    "N_batch = 100  # size of the minibatch\n",
    "prec = 1e-30\n",
    "delta = 0.4    # Strength of the anti-hebbian learning\n",
    "p = 2.0        # Lebesgue norm of the weights\n",
    "k = 2          # ranking parameter, must be integer that is bigger or equal than 2\n",
    "\n",
    "N_in = 784     # input units\n",
    "Nc = 10        # number of classes\n",
    "val_split = 1/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a utility function to shuffle training and testing samples\n",
    "def shuffle(xt, yt, xv, yv):\n",
    "    \n",
    "    # xt, yt: training samples and labels\n",
    "    # xv, yv: validation samples and labels\n",
    "    \n",
    "    pt = np.random.permutation(len(xt))\n",
    "    pv = np.random.permutation(len(xv))\n",
    "    return(xt[pt], yt[pt], xv[pv], yv[pv])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Let's load the data from `keras`:\n",
    "- Use `keras.datasets.mnist.load_data()` to load the 60000 train and 10000 test samples.\n",
    "- Shuffle them\n",
    "- Normalize to \\[0, 1\\] range\n",
    "- Reshape images to (#samples, 784) and labels to (#samples, 10)\n",
    "- Split the train part to 50000 train and 10000 validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell derived from this page: https://keras.io/examples/vision/mnist_convnet/\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# reshape to (# samples, 784)\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1]* x_train.shape[2])\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1]* x_test.shape[2])\n",
    "\n",
    "# shuffle\n",
    "x_train, y_train, x_test, y_test = shuffle(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# normalize\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# indices of validation splilt\n",
    "val_idx = np.random.choice(x_train.shape[0], int(val_split * x_train.shape[0]), replace=False)\n",
    "\n",
    "#split validation\n",
    "x_val = x_train[val_idx]\n",
    "y_val = y_train[val_idx]\n",
    "x_train = np.delete(x_train, val_idx, axis=0)\n",
    "y_train = np.delete(y_train, val_idx, axis=0)\n",
    "\n",
    "# some logging\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x_val shape:\", x_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, Nc)\n",
    "y_val = keras.utils.to_categorical(y_val, Nc)\n",
    "y_test = keras.utils.to_categorical(y_test, Nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = x_train.shape[0]\n",
    "N_val = x_val.shape[0]\n",
    "N_test = x_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights visualization\n",
    "To draw a heatmap of the weights a helper function is created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_weights(synapses, Kx, Ky, ax=None):\n",
    "    # synapses: the weights\n",
    "    Kw = int(np.sqrt(synapses.shape[1]))\n",
    "    yy=0\n",
    "    HM=np.zeros((Kw*Ky,Kw*Kx))\n",
    "    for y in range(Ky):\n",
    "        for x in range(Kx):\n",
    "            HM[y*Kw:(y+1)*Kw,x*Kw:(x+1)*Kw]=synapses[yy,:].reshape(Kw,Kw)\n",
    "            yy += 1\n",
    "   \n",
    "    nc=np.amax(np.absolute(HM))\n",
    "    \n",
    "    if ax is not None:\n",
    "        im = ax.imshow(HM, cmap='bwr', vmin=-nc, vmax = nc)\n",
    "        fig.colorbar(im, ticks=[np.amin(HM), 0, np.amax(HM)], ax=ax)\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        im=plt.imshow(HM,cmap='bwr',vmin=-nc,vmax=nc)\n",
    "        fig.colorbar(im,ticks=[np.amin(HM), 0, np.amax(HM)])\n",
    "        plt.axis('off')\n",
    "    fig.canvas.draw() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass of the unsupervised part\n",
    "This is one step of the training and returns the changes that should be applied to the weights in one iteration. \n",
    "\n",
    "**Note:** In the testing phase, we don't need to compute `h` or `g` functions, so we just return the input currents.\n",
    "- Given the inputs and the synapses (weights), compute the input currents `tot_input` = <W.v>\n",
    "    - This is referred to `I` in the paper.\n",
    "- If we're in the testing phase, return `tot_input`\n",
    "- If we're in the training phase, go on to use the rankings in the input currents as proxies for the final `h` and return the changes that should be application "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inputs, synapses, p, N_batch, training=True):\n",
    "    N_hid, N_in = synapses.shape\n",
    "    inputs = np.transpose(inputs)\n",
    "    sig=np.sign(synapses)\n",
    "    tot_input=np.dot(sig*np.absolute(synapses)**(p-1),inputs) # with p=2, this is equal to <W.v> = I \n",
    "    \n",
    "\n",
    "    if training == False:\n",
    "        return tot_input.T\n",
    "    \n",
    "    # TODO: compute h...\n",
    "    \n",
    "    y=np.argsort(tot_input,axis=0) # using tot_input (I) as proxy for h\n",
    "    yl=np.zeros((N_hid, N_batch)) # y1 = g(Q)\n",
    "    yl[y[N_hid-1],np.arange(N_batch)]=1.0 # g(max_activation in I) = 1\n",
    "    yl[y[N_hid-k],np.arange(N_batch)]=-delta # g(second max activation) = -0.4\n",
    "\n",
    "    xx=np.sum(np.multiply(yl,tot_input),1) # g(Q) x <W, v>\n",
    "    ds=np.dot(yl,np.transpose(inputs)) - np.multiply(np.tile(xx.reshape(xx.shape[0],1),(1,N_in)),synapses)\n",
    "    # g(Q) (v_i - <W,v> W_i)\n",
    "    nc=np.amax(np.absolute(ds))\n",
    "    return ds, nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The unsupervised training of the hidden layer\n",
    "This cell defines the main code. The external loop runs over epochs `nep`, the internal loop runs over minibatches. For every minibatch the overlap with the data `tot_input` is calculated for each data point and each hidden unit. The sorted strengths of the activations are stored in `y`. The variable `yl` stores the activations of the post synaptic cells - it is denoted by g(Q) in Eq 3 of [Unsupervised Learning by Competing Hidden Units](https://doi.org/10.1073/pnas.1820458116), see also Eq 9 and Eq 10. The variable `ds` is the right hand side of Eq 3. The weights are updated after each minibatch in a way so that the largest update is equal to the learning rate `eps` at that epoch. The weights are displayed by the helper function after each epoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "This will take some time, especially if the hidden layer is big (e.g. N_hid>500). If you want to see a demo, skip to the next cell to load the pre-trained weights.\n",
    "\n",
    "If you do train the model yourself, make sure to comment the next cell so you don't replace the your trained weights with the pre-trained ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "fig=plt.figure(figsize=(12.9,10))\n",
    "\n",
    "def train_weights(x_train, N_hid, N_batch, Nep=Nep, mu=0, sigma=1):\n",
    "    N_train, N_in = x_train.shape\n",
    "    synapses = np.random.normal(mu, sigma, (N_hid, N_in)) # W\n",
    "    print(synapses.shape)\n",
    "    for nep in range(Nep):\n",
    "        eps=eps0*(1-nep/Nep)\n",
    "        for i in range(N_train//N_batch):\n",
    "            inputs=x_train[i*N_batch:(i+1)*N_batch,:] # v_i \n",
    "            ds, nc = forward(inputs, synapses, p, N_batch)\n",
    "            if nc<prec:\n",
    "                nc=prec\n",
    "            synapses += eps*np.true_divide(ds,nc)\n",
    "\n",
    "        if nep%20 == 0:    \n",
    "            print('epoch ' + str(nep))\n",
    "        draw_weights(synapses, Kx, Ky)\n",
    "    return synapses\n",
    "synapses = train_weights(x_train, N_hid, N_batch, Nep)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving or loading synapses\n",
    "- If you want to save your own synapses from previous cell, uncomment and edit the first line\n",
    "- If you want to load a pre-trained model, use the `np.load` function in the second line\n",
    "\n",
    "A sample `synapses.npy` file can be found [here](https://drive.google.com/file/d/1ZEHI4yCa8ZqrwySM5_3jy-bcIenlUuA4/view?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your own model\n",
    "# np.save('synapses_hid1000_epoch1000_eps4e-2_p2_k2_batch100.npy', synapses)\n",
    "# np.save('synapses_hid100_epoch150_eps4e-2_p2_k2_batch100.npy', synapses)\n",
    "\n",
    "# load a pre-trained model\n",
    "synapses = np.load('synapses_hid1000_epoch1000_eps4e-2_p2_k2_batch100.npy')\n",
    "# synapses = np.load('synapses_hid100_epoch150_eps4e-2_p2_k2_batch100.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "fig=plt.figure(figsize=(10, 7))\n",
    "draw_weights(synapses, Kx, Ky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    x_norm = x - x.mean(axis=1, keepdims=True)\n",
    "    x_norm = x_norm / np.linalg.norm(x_norm, ord=2, axis=1, keepdims=True)\n",
    "    return x_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing: compute hidden neurons for train, val, and test data\n",
    "This will be the input to the second layer, which will be trained with SGD to do digit recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center and normalize input data to unit vectors  \n",
    "x_train0 = normalize(x_train)\n",
    "x_val0 = normalize(x_val)\n",
    "x_test0 = normalize(x_test)\n",
    "\n",
    "x_hid_train = forward(x_train0, synapses, p, N_train, training=False)\n",
    "x_hid_val = forward(x_val0, synapses, p, N_val, training=False)\n",
    "x_hid_test = forward(x_test0, synapses, p, N_val, training=False)\n",
    "\n",
    "n=4.5\n",
    "x_hid_train = (x_hid_train * (x_hid_train>0)) ** n\n",
    "x_hid_val = (x_hid_val * (x_hid_val>0)) ** n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the BIO model\n",
    "- Input is the pre-computed hidden activations.\n",
    "- Output layer is a fully connected layer with softmax activation\n",
    "- Loss function is the categorical cross-entropy\n",
    "- Optimizer is Adam with initial learning rate of 0.1 which decays exponentially every 20 epochs with a decay rate of 0.7\n",
    "- Batch size is 100 and number of epochs is 100\n",
    "\n",
    "Set `verbose=1` in the `model.fit()` function to see the loss and accuracy values after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_model = keras.Sequential([\n",
    "    layers.Input(shape=(N_hid,)),\n",
    "    layers.Dense(Nc),\n",
    "    layers.Activation('softmax')\n",
    "])\n",
    "\n",
    "print(bio_model.summary())\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-1, decay_steps=Nep*20, decay_rate=0.7, staircase=True)\n",
    "opt = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "bio_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "bio_logs = bio_model.fit(x_hid_train, y_train, validation_data=(x_hid_val, y_val), batch_size=100, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the backprop model\n",
    "- One fully-connected hidden layer with the same number of neurons as the BIO model, i.e. `N_hid`\n",
    "    - Activation is 'relu'\n",
    "- Another fully-connected output layer with softmax activation\n",
    "- Same loss and optimizer as BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_model = keras.Sequential([\n",
    "    layers.Input(shape=(N_in,)),\n",
    "    layers.Dense(N_hid),\n",
    "#     layers.BatchNormalization(),\n",
    "    layers.Activation(\"relu\"),\n",
    "    layers.Dense(Nc, activation=\"softmax\")\n",
    "])\n",
    "bp_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "bp_logs = bp_model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=100, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot losses and accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_history = bio_logs.history\n",
    "bp_history = bp_logs.history\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(bio_history['loss'])\n",
    "plt.plot(bio_history['val_loss'])\n",
    "\n",
    "plt.plot(bp_history['loss'])\n",
    "plt.plot(bp_history['val_loss'])\n",
    "\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "# plt.ylim(0, 1)\n",
    "plt.xlim(0, 50)\n",
    "plt.legend(['Train_BIO', 'Validation_BIO', 'Train_BP', 'Validation_BP'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(bio_history['accuracy'])\n",
    "plt.plot(bio_history['val_accuracy'])\n",
    "\n",
    "plt.plot(bp_history['accuracy'])\n",
    "plt.plot(bp_history['val_accuracy'])\n",
    "\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim(.9, 1)\n",
    "plt.xlim(0, 50)\n",
    "plt.legend(['Train_BIO', 'Validation_BIO', 'Train_BP', 'Validation_BP'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('BIO best validation accuracy:', max(bio_history['val_accuracy']))\n",
    "print('BP best validation accuracy:', max(bp_history['val_accuracy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot weights of the BP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(13, 5))\n",
    "ax[0].set_title('BP')\n",
    "ax[1].set_title('BIO')\n",
    "bp_weights = bp_model.layers[0].get_weights()[0].T\n",
    "draw_weights(bp_weights, Kx, Ky, ax[0])\n",
    "draw_weights(synapses, Kx, Ky, ax[1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control Test: A model with only one output layer\n",
    "If a model with only one output layer performs on paar with BIO, this means the images themselves are as representative as the biological hidden neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_model = keras.Sequential([\n",
    "    layers.Input(shape=(N_in)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(Nc, activation='softmax')\n",
    "])\n",
    "control_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "control_logs = control_model.fit(x_train0, y_train, validation_data=(x_val0, y_val), batch_size=100, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Training Accuracy:\", control_logs.history['accuracy'][-1])\n",
    "print(\"Final Validation Accuracy:\", control_logs.history['val_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a second hidden layer\n",
    "The goal is to have two hidden layers, each having 100 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inputs, synapses, p, N_hid, N_batch, training=True):\n",
    "    inputs = np.transpose(inputs)\n",
    "    sig=np.sign(synapses)\n",
    "    tot_input=np.dot(sig*np.absolute(synapses)**(p-1),inputs) # with p=2, this is equal to <W.v> = I \n",
    "    \n",
    "\n",
    "    if training == False:\n",
    "        return tot_input.T\n",
    "    \n",
    "    # TODO: compute h...\n",
    "    \n",
    "    y=np.argsort(tot_input,axis=0) # using tot_input (I) as proxy for h\n",
    "    yl=np.zeros((N_hid, N_batch)) # y1 = g(Q)\n",
    "    yl[y[N_hid-1],np.arange(N_batch)]=1.0 # g(max_activation in I) = 1\n",
    "    yl[y[N_hid-k],np.arange(N_batch)]=-delta # g(second max activation) = -0.4\n",
    "#     if training == False:\n",
    "#         return yl.T\n",
    "    xx=np.sum(np.multiply(yl,tot_input),1) # g(Q) x <W, v>\n",
    "#     print(inputs.shape)\n",
    "#     print(np.dot(yl, np.transpose(inputs)).shape)\n",
    "#     print(np.tile(xx.reshape(xx.shape[0],1),(1,N_in)).shape)\n",
    "#     print(np.multiply(np.tile(xx.reshape(xx.shape[0],1),(1,N_in)),synapses).shape)\n",
    "    ds=np.dot(yl,np.transpose(inputs)) - np.multiply(np.tile(xx.reshape(xx.shape[0],1),(1,N_hid)),synapses)\n",
    "    # g(Q) (v_i - <W,v> W_i)\n",
    "    nc=np.amax(np.absolute(ds))\n",
    "    return ds, nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "eps0 = 2e-2\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "plt.tight_layout()\n",
    "# print(x_hid_train.shape)\n",
    "synapses_2 = train_weights(x_hid_train, N_hid, N_batch=N_batch, Nep=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center and normalize input data to unit vectors  \n",
    "x_train1 = normalize(x_hid_train)\n",
    "x_val1 = normalize(x_hid_val)\n",
    "x_test1 = normalize(x_hid_test)\n",
    "\n",
    "x_hid_train2 = forward(x_train1, synapses_2, p, N_train, training=False)\n",
    "x_hid_val2 = forward(x_val1, synapses_2, p, N_val, training=False)\n",
    "x_hid_test2 = forward(x_test1, synapses_2, p, N_val, training=False)\n",
    "\n",
    "n=2\n",
    "x_hid_train2 = (x_hid_train2 * (x_hid_train2>0)) ** n\n",
    "x_hid_val2 = (x_hid_val2 * (x_hid_val2>0)) ** n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_model2 = keras.Sequential([\n",
    "    layers.Input(shape=(N_hid,)),\n",
    "    layers.Dense(Nc),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('softmax')\n",
    "])\n",
    "print(bio_model2.summary())\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-1, decay_steps=Nep*20, decay_rate=0.7, staircase=True)\n",
    "opt = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "bio_model2.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "bio_logs2 = bio_model2.fit(x_hid_train2, y_train, validation_data=(x_hid_val2, y_val), batch_size=100, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "**Update 19.12.2021**: We couldn't reproduce the paper results with 100 hidden neurons. We noticed that the paper uses 2000 hidden neurons, so we used a hidden layer of size 1000 (since 2000 would take too much time). This took about 12 hours to train on my local machine (Intel Core-i7-4702MQ, 16GB RAM). This is pretty much impractical and incomparable with the training of the BP-based model in terms of efficiency. One idea is to see if two hidden layers of lower size will increase accuracy while lowering the training time.  \n",
    "\n",
    "### short-term \n",
    "- ~~load data from keras (MNIST)~~\n",
    "- ~~add a softmax layer to train the netwok (SGD, ...)~~\n",
    "- ~~get the network run on a simple validation set~~\n",
    "- ~~get the accuracy ...~~\n",
    "- fine-tune bio model to get the best result\n",
    "- add a second hidden layer\n",
    "\n",
    "### long-term\n",
    "- ~~get the result from the bio network~~\n",
    "- ~~train a \"usual\" net~~\n",
    "- ~~compare the results~~\n",
    "- ~~visualize the weights in the backprop-based network~~\n",
    "- do the bio computation on gpu?\n",
    "- ..."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
