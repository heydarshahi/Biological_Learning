{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code illustrates the fast AI implementation of the unsupervised \"biological\" learning algorithm from [Unsupervised Learning by Competing Hidden Units](https://doi.org/10.1073/pnas.1820458116) on MNIST data set. \n",
    "If you want to learn more about this work you can also check out this [lecture](https://www.youtube.com/watch?v=4lY-oAY0aQU) from MIT's [6.S191 course](http://introtodeeplearning.com/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell loads the data and normalizes it to the [0,1] range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps0=4e-2    # learning rate\n",
    "Kx=10\n",
    "Ky=10\n",
    "hid_disp=Kx*Ky    # number of hidden units that are displayed in Ky by Kx array\n",
    "N_hid = 1000  # number of hidden units\n",
    "mu=0.0\n",
    "sigma=1.0\n",
    "Nep=1000      # number of epochs\n",
    "N_batch=100      # size of the minibatch\n",
    "prec=1e-30\n",
    "delta=0.4    # Strength of the anti-hebbian learning\n",
    "p=2.0        # Lebesgue norm of the weights\n",
    "k=2          # ranking parameter, must be integer that is bigger or equal than 2\n",
    "\n",
    "Nc = 10\n",
    "N_in = 784\n",
    "Nc = 10\n",
    "val_split = 1/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(xt, yt, xv, yv):\n",
    "    pt = np.random.permutation(len(xt))\n",
    "    pv = np.random.permutation(len(xv))\n",
    "    return(xt[pt], yt[pt], xv[pv], yv[pv])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Let's look at the `mat` matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell derived from this page: https://keras.io/examples/vision/mnist_convnet/\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1]* x_train.shape[2])\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1]* x_test.shape[2])\n",
    "\n",
    "x_train, y_train, x_test, y_test = shuffle(x_train, y_train, x_test, y_test)\n",
    "\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "val_idx = np.random.choice(x_train.shape[0], int(val_split * x_train.shape[0]), replace=False)\n",
    "\n",
    "x_val = x_train[val_idx]\n",
    "y_val = y_train[val_idx]\n",
    "x_train = np.delete(x_train, val_idx, axis=0)\n",
    "y_train = np.delete(y_train, val_idx, axis=0)\n",
    "\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "# x_train = np.expand_dims(x_train, -1)\n",
    "# x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x_val shape:\", x_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, Nc)\n",
    "y_val = keras.utils.to_categorical(y_val, Nc)\n",
    "y_test = keras.utils.to_categorical(y_test, Nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = x_train.shape[0]\n",
    "N_val = x_val.shape[0]\n",
    "N_test = x_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To draw a heatmap of the weights a helper function is created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_weights(synapses, Kx, Ky):\n",
    "    yy=0\n",
    "    HM=np.zeros((28*Ky,28*Kx))\n",
    "    for y in range(Ky):\n",
    "        for x in range(Kx):\n",
    "            HM[y*28:(y+1)*28,x*28:(x+1)*28]=synapses[yy,:].reshape(28,28)\n",
    "            yy += 1\n",
    "    plt.clf()\n",
    "    nc=np.amax(np.absolute(HM))\n",
    "    im=plt.imshow(HM,cmap='bwr',vmin=-nc,vmax=nc)\n",
    "    fig.colorbar(im,ticks=[np.amin(HM), 0, np.amax(HM)])\n",
    "    plt.axis('off')\n",
    "    fig.canvas.draw()   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines paramaters of the algorithm: `eps0` - initial learning rate that is linearly annealed during training; `hid` - number of hidden units that are displayed as an `Ky` by `Kx` array by the helper function defined above; `mu` - the mean of the gaussian distribution that initializes the weights; `sigma` - the standard deviation of that gaussian; `Nep` - number of epochs; `Num` - size of the minibatch; `prec` - parameter that controls numerical precision of the weight updates; `delta` - the strength of the anti-hebbian learning; `p` - Lebesgue norm of the weights; `k` - ranking parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines the main code. The external loop runs over epochs `nep`, the internal loop runs over minibatches. For every minibatch the overlap with the data `tot_input` is calculated for each data point and each hidden unit. The sorted strengths of the activations are stored in `y`. The variable `yl` stores the activations of the post synaptic cells - it is denoted by g(Q) in Eq 3 of [Unsupervised Learning by Competing Hidden Units](https://doi.org/10.1073/pnas.1820458116), see also Eq 9 and Eq 10. The variable `ds` is the right hand side of Eq 3. The weights are updated after each minibatch in a way so that the largest update is equal to the learning rate `eps` at that epoch. The weights are displayed by the helper function after each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inputs, synapses, p, N_hid, N_batch, training=True):\n",
    "    inputs = np.transpose(inputs)\n",
    "    sig=np.sign(synapses)\n",
    "    tot_input=np.dot(sig*np.absolute(synapses)**(p-1),inputs) # with p=2, this is equal to <W.v> = I \n",
    "    y=np.argsort(tot_input,axis=0) # using tot_input (I) as proxy for h\n",
    "\n",
    "    if training == False:\n",
    "        return tot_input.T\n",
    "    \n",
    "    # TODO: compute h...\n",
    "    \n",
    "    yl=np.zeros((N_hid, N_batch)) # y1 = g(Q)\n",
    "    yl[y[N_hid-1],np.arange(N_batch)]=1.0 # g(max_activation in I) = 1\n",
    "    yl[y[N_hid-k],np.arange(N_batch)]=-delta # g(second max activation) = -0.4\n",
    "#     if training == False:\n",
    "#         return yl.T\n",
    "    xx=np.sum(np.multiply(yl,tot_input),1) # g(Q) x <W, v>\n",
    "    ds=np.dot(yl,np.transpose(inputs)) - np.multiply(np.tile(xx.reshape(xx.shape[0],1),(1,N_in)),synapses)\n",
    "    # g(Q) (v_i - <W,v> W_i)\n",
    "    nc=np.amax(np.absolute(ds))\n",
    "    return ds, nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "fig=plt.figure(figsize=(12.9,10))\n",
    "\n",
    "synapses = np.random.normal(mu, sigma, (N_hid, N_in)) # W\n",
    "print(synapses.shape)\n",
    "for nep in range(Nep):\n",
    "    eps=eps0*(1-nep/Nep)\n",
    "#     M=M[np.random.permutation(N_train),:]\n",
    "    for i in range(N_train//N_batch):\n",
    "        inputs=x_train[i*N_batch:(i+1)*N_batch,:] # v_i \n",
    "        ds, nc = forward(inputs, synapses, p, N_hid, N_batch)\n",
    "        if nc<prec:\n",
    "            nc=prec\n",
    "        synapses += eps*np.true_divide(ds,nc)\n",
    "        \n",
    "    if nep%20 == 0:    \n",
    "        print('epoch ' + str(nep))\n",
    "    draw_weights(synapses, Kx, Ky)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('synapses_hid1000_epoch1000_eps4e-2_p2_k2_batch100.npy', synapses)\n",
    "# synapses = np.load('synapses.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "fig=plt.figure()\n",
    "draw_weights(synapses, Kx, Ky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train0 = x_train - x_train.mean(axis=1, keepdims=True) \n",
    "x_val0 = x_val - x_val.mean(axis=1, keepdims=True)\n",
    "x_hid_train = forward(x_train0/np.linalg.norm(x_train0, ord=2, axis=1, keepdims=True), synapses, p, N_hid, N_train, training=False)\n",
    "x_hid_val = forward(x_val0/np.linalg.norm(x_val0, axis=1, ord=2, keepdims=True), synapses, p, N_hid, N_val, training=False)\n",
    "\n",
    "# x_hid_train /= np.linalg.norm(x_hid_train, ord=2, axis=1, keepdims=True)\n",
    "# x_hid_val /= np.linalg.norm(x_hid_train, ord=2, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "# x_hid_train = forward(x_train, synapses, p, N_hid, N_train, training=False)\n",
    "# x_hid_val = forward(x_val, synapses, p, N_hid, N_val, training=False)\n",
    "# x_hid_test = forward(x_test, synapses, p, N_hid, N_test, training=False)\n",
    "\n",
    "print(\"x_hid_train shape:\", x_hid_train.shape)\n",
    "print(\"x_hid_val shape:\", x_hid_val.shape)\n",
    "# print(\"x_hid_test shape:\", x_hid_test.shape)\n",
    "print(x_hid_val.max())\n",
    "\n",
    "n=4.5\n",
    "x_hid_train = (x_hid_train * (x_hid_train>0)) ** n\n",
    "x_hid_val = (x_hid_val * (x_hid_val>0)) ** n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=4.5\n",
    "x_hid_train = (x_hid_train * (x_hid_train>0)) ** n\n",
    "x_hid_val = (x_hid_val * (x_hid_val>0)) ** n\n",
    "# x_hid_test = (x_hid_test * (x_hid_test>0)) ** n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_model = keras.Sequential([\n",
    "    layers.Input(shape=(N_hid,)),\n",
    "#     layers.Activation('elu'),\n",
    "#     layers.BatchNormalization(),\n",
    "    layers.Dense(Nc),\n",
    "#     layers.BatchNormalization(),\n",
    "    layers.Activation('softmax')\n",
    "])\n",
    "# print(bio_model.summary())\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-1, decay_steps=Nep*20, decay_rate=0.7, staircase=True)\n",
    "opt = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "bio_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "bio_logs = bio_model.fit(x_hid_train, y_train, validation_data=(x_hid_val, y_val), batch_size=100, epochs=300, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_model = keras.Sequential([\n",
    "    layers.Input(shape=(N_in,)),\n",
    "    layers.Dense(N_hid),\n",
    "#     layers.BatchNormalization(),\n",
    "    layers.Activation(\"relu\"),\n",
    "    layers.Dense(Nc, activation=\"softmax\")\n",
    "])\n",
    "bp_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "bp_logs = bp_model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=100, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_history = bio_logs.history\n",
    "bp_history = bp_logs.history\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(bio_history['loss'])\n",
    "plt.plot(bio_history['val_loss'])\n",
    "\n",
    "plt.plot(bp_history['loss'])\n",
    "plt.plot(bp_history['val_loss'])\n",
    "\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "# plt.ylim(0, 1)\n",
    "plt.xlim(0, 50)\n",
    "plt.legend(['Train_BIO', 'Validation_BIO', 'Train_BP', 'Validation_BP'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(bio_history['accuracy'])\n",
    "plt.plot(bio_history['val_accuracy'])\n",
    "\n",
    "plt.plot(bp_history['accuracy'])\n",
    "plt.plot(bp_history['val_accuracy'])\n",
    "\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim(.9, 1)\n",
    "plt.xlim(0, 50)\n",
    "plt.legend(['Train_BIO', 'Validation_BIO', 'Train_BP', 'Validation_BP'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control Test: A model with only one output layer\n",
    "If a model with only one output layer performs on paar with BIO, this means the images themselves are as representative as the biological hidden neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_model = keras.Sequential([\n",
    "    layers.Input(shape=(N_in)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(Nc, activation='softmax')\n",
    "])\n",
    "control_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "control_logs = control_model.fit(x_train0, y_train, validation_data=(x_val0, y_val), batch_size=100, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Training Accuracy:\", control_logs.history['accuracy'][-1])\n",
    "print(\"Final Validation Accuracy:\", control_logs.history['val_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### short-term \n",
    "- load data from keras (MNIST)\n",
    "- add a softmax layer to train the netwok (SGD, ...)\n",
    "- get the network run on a simple validation set\n",
    "- get the accuracy ...\n",
    "\n",
    "### long-term\n",
    "- get the result from the bio network\n",
    "- transfer to gpu?\n",
    "- train a \"usual\" net\n",
    "- compare the results \n",
    "- visualize the weights in the backprop-based network\n",
    "- do the bio computation on gpu?\n",
    "- ..."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
