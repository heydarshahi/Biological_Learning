{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code illustrates the fast AI implementation of the unsupervised \"biological\" learning algorithm from [Unsupervised Learning by Competing Hidden Units](https://doi.org/10.1073/pnas.1820458116) on CIFAR-10 data set. \n",
    "If you want to learn more about this work you can also check out this [lecture](https://www.youtube.com/watch?v=4lY-oAY0aQU) from MIT's [6.S191 course](http://introtodeeplearning.com/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines paramaters of the algorithm: \n",
    "- `eps0`: initial learning rate that is linearly annealed during training\n",
    "- `N_hid`: number of hidden units that are displayed as an `Ky` by `Kx` array by the `draw_weights` function defined below\n",
    "- `mu`: the mean of the gaussian distribution that initializes the weights\n",
    "- `sigma`: the standard deviation of that gaussian \n",
    "- `Nep`: number of epochs\n",
    "- `N_batch`: size of the minibatch\n",
    "- `prec`: parameter that controls numerical precision of the weight updates\n",
    "- `delta`: the strength of the anti-hebbian learning\n",
    "- `p`: Lebesgue norm of the weights; `k` - ranking parameter. \n",
    "- `N_in`: number of input units\n",
    "- `val_split`: the proportion of validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps0 = 3e-2    # initial learning rate for the unsupervised part\n",
    "Kx = 10\n",
    "Ky = 10\n",
    "hid_disp = Kx*Ky    # number of hidden units that are displayed in Ky by Kx array\n",
    "N_hid = 2000    # number of hidden units\n",
    "mu = 0.0\n",
    "sigma = 1.0\n",
    "Nep = 1000     # number of epochs\n",
    "N_batch = 1000  # size of the minibatch\n",
    "prec = 1e-30\n",
    "delta = 0.2    # Strength of the anti-hebbian learning\n",
    "p = 2.0        # Lebesgue norm of the weights\n",
    "k = 2          # ranking parameter, must be integer that is bigger or equal than 2\n",
    "\n",
    "N_in = 3072     # input units\n",
    "Nc = 10        # number of classes\n",
    "val_split = 1/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuda Parameters\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a utility function to shuffle training and testing samples\n",
    "def shuffle(xt, yt, xv, yv):\n",
    "    \n",
    "    # xt, yt: training samples and labels\n",
    "    # xv, yv: validation samples and labels\n",
    "    \n",
    "    pt = np.random.permutation(len(xt))\n",
    "    pv = np.random.permutation(len(xv))\n",
    "    return(xt[pt], yt[pt], xv[pv], yv[pv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move data from and to gpu\n",
    "\n",
    "def detach(tensor):\n",
    "    try:\n",
    "        return tensor.cpu().detach().numpy()\n",
    "    except:\n",
    "        return tensor\n",
    "    \n",
    "def attach(tensor):\n",
    "    try:\n",
    "        return torch.from_numpy(tensor).float().to(device)\n",
    "    except:\n",
    "        print('could not move data to cuda')\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Let's load the data from `keras`:\n",
    "- Use `keras.datasets.cifar10.load_data()` to load the 45000 train and 5000 test samples.\n",
    "- Reshape images to (#samples, 3x32x32) and labels to (#samples, 10)\n",
    "- Split the train part to 45000 and 5000 validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "print(x_train.shape)\n",
    "# reshape to (# samples, 784)\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1] * x_train.shape[2] * x_train.shape[3])\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2] * x_test.shape[3])\n",
    "\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "x_train -= x_train.mean(axis=0, keepdims=True) \n",
    "x_test -= x_test.mean(axis=0, keepdims=True)\n",
    "x_train /= np.linalg.norm(x_train, ord=2, axis=1, keepdims=True)\n",
    "x_test /= np.linalg.norm(x_test, ord=2, axis=1, keepdims=True)\n",
    "\n",
    "# shuffle\n",
    "x_train, y_train, x_test, y_test = shuffle(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# indices of validation splilt\n",
    "val_idx = np.random.choice(x_train.shape[0], int(val_split * x_train.shape[0]), replace=False)\n",
    "\n",
    "#split validation\n",
    "x_val = x_train[val_idx]\n",
    "y_val = y_train[val_idx]\n",
    "x_train = np.delete(x_train, val_idx, axis=0)\n",
    "y_train = np.delete(y_train, val_idx, axis=0)\n",
    "\n",
    "# some logging\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x_val shape:\", x_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, Nc)\n",
    "y_val = keras.utils.to_categorical(y_val, Nc)\n",
    "y_test = keras.utils.to_categorical(y_test, Nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = x_train.shape[0]\n",
    "N_val = x_val.shape[0]\n",
    "N_test = x_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights visualization\n",
    "To draw a heatmap of the weights a helper function is created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_weights(synapses, Kx, Ky, ax=None):\n",
    "    # synapses: the weights\n",
    "    Kw = int(np.sqrt(synapses.shape[1]//3)) # i.e. 32\n",
    "    yy=0\n",
    "    HM=np.zeros((Kw*Ky, Kw*Kx, 3))\n",
    "    for y in range(Ky):\n",
    "        for x in range(Kx):\n",
    "            HM[y*Kw:(y+1)*Kw,x*Kw:(x+1)*Kw]=synapses[yy,:Kw*Kw*3].reshape(Kw, Kw, 3)\n",
    "            yy += 1\n",
    "   \n",
    "    nc=np.amax(np.absolute(HM))\n",
    "    tmp = (HM-HM.min())\n",
    "    tmp /= tmp.max() \n",
    "    tmp *= 255\n",
    "    tmp = tmp.astype(np.uint8)\n",
    "    if ax is not None:\n",
    "        im = ax.imshow(tmp)\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        im=plt.imshow(tmp.astype(np.uint8))\n",
    "        plt.axis('off')\n",
    "    fig.canvas.draw() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass of the unsupervised part\n",
    "This is one step of the training and returns the changes that should be applied to the weights in one iteration. \n",
    "\n",
    "**Note:** In the testing phase, we don't need to compute `h` or `g` functions, so we just return the input currents.\n",
    "- Given the inputs and the synapses (weights), compute the input currents `tot_input` = <W.v>\n",
    "    - This is referred to `I` in the paper.\n",
    "- If we're in the testing phase, return `tot_input`\n",
    "- If we're in the training phase, go on to use the rankings in the input currents as proxies for the final `h` and return the changes that should be application "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inputs, synapses, p, N_hid, N_batch, training=True):\n",
    "    N_in = inputs.shape[1]\n",
    "    inputs = torch.transpose(inputs, 0, 1)\n",
    "    sig = torch.sign(synapses).to(device)\n",
    "    \n",
    "    # with p=2, this is equal to <W.v> = I \n",
    "    tot_input = torch.mm(sig*torch.abs(synapses).pow(p-1), inputs)\n",
    "    \n",
    "    if training == False:\n",
    "        return torch.transpose(tot_input, 0, 1)\n",
    "    \n",
    "    y = torch.argsort(tot_input, dim=0).to(device) # using tot_input (I) as proxy for h\n",
    "    y1 = torch.zeros((N_hid, N_batch)).to(device)  # g(Q)\n",
    "    y1[y[N_hid-1, :], np.arange(N_batch)] = 1.0    # g(max_activation in I) = 1\n",
    "    y1[y[N_hid-k], np.arange(N_batch)] = -delta    # g(second max activation) = -delta\n",
    "\n",
    "\n",
    "    xx = torch.sum(torch.mul(y1, tot_input), 1)    # g(Q) * <W, v>\n",
    "    ds = torch.matmul(y1, torch.transpose(inputs, 0, 1)) - torch.mul(xx.reshape(xx.shape[0],1).repeat(1, N_in), synapses)\n",
    "    nc = torch.max(torch.abs(ds))\n",
    "\n",
    "    return ds, nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The unsupervised training of the hidden layer\n",
    "This cell defines the main code. The external loop runs over epochs `nep`, the internal loop runs over minibatches. For every minibatch the overlap with the data `tot_input` is calculated for each data point and each hidden unit. The sorted strengths of the activations are stored in `y`. The variable `yl` stores the activations of the post synaptic cells - it is denoted by g(Q) in Eq 3 of [Unsupervised Learning by Competing Hidden Units](https://doi.org/10.1073/pnas.1820458116), see also Eq 9 and Eq 10. The variable `ds` is the right hand side of Eq 3. The weights are updated after each minibatch in a way so that the largest update is equal to the learning rate `eps` at that epoch. The weights are displayed by the helper function after each epoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "This will take some time, especially if the hidden layer is big (e.g. N_hid>500). If you want to see a demo, skip to the next cell to load the pre-trained weights.\n",
    "\n",
    "If you do train the model yourself, make sure to comment the next cell so you don't replace the your trained weights with the pre-trained ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_weights(x_train, N_hid, N_batch, Nep=Nep, mu=0, sigma=1, device=\"cpu\"):\n",
    "\n",
    "    N_train, N_in = x_train.shape\n",
    "    x_train = attach(x_train)\n",
    "    synapses = (torch.randn(N_hid, N_in) * sigma + mu).to(device)\n",
    "    eps0 = torch.tensor(4e-2).to(device)\n",
    "\n",
    "    for nep in tqdm(range(Nep)):\n",
    "        eps=eps0*(1-nep/Nep)\n",
    "\n",
    "        for i in range(N_train//N_batch):\n",
    "            inputs=x_train[i*N_batch:(i+1)*N_batch,:] # v_i \n",
    "            ds, nc = forward(inputs, synapses, p, N_hid, N_batch)\n",
    "            \n",
    "            if nc < prec:\n",
    "                nc = prec\n",
    "            synapses += torch.mul(torch.div(ds, nc), eps)        \n",
    "  \n",
    "        draw_weights(detach(synapses), Kx, Ky)\n",
    "    \n",
    "    return synapses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig=plt.figure(figsize=(12.9,10))\n",
    "print('using device: {}'.format(device))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "synapses = train_weights(x_train, N_hid, N_batch, Nep, device=device).to(device)\n",
    "\n",
    "print('took {:.4f}s to run {} epochs'.format(time.time() - start, Nep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving or loading synapses\n",
    "- If you want to save your own synapses from previous cell, uncomment and edit the first line\n",
    "- If you want to load a pre-trained model, use the `np.load` function in the second line\n",
    "\n",
    "A sample `synapses.npy` file can be found [here](https://drive.google.com/file/d/1b-2FNqVQ1fN-eixeTO9WZin9eNl2gqou/view?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your own model\n",
    "np.save('CIFAR10_synapses_hid{}_epoch{}_eps{}_p2_k2_batch{}.npy'.format(N_hid, Nep, eps0, N_batch), detach(synapses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pre-trained model\n",
    "synapses = np.load('CIFAR10_synapses_hid{}_epoch{}_eps{}_p2_k2_batch{}.npy'.format(N_hid, Nep, eps0, N_batch), detach(synapses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "%matplotlib notebook\n",
    "fig=plt.figure(figsize=(10, 7))\n",
    "draw_weights(detach(synapses), Kx*2, Ky*2)\n",
    "plt.tight_layout()\n",
    "plt.savefig('CIFAR10_synapses_hid{}_epoch{}_eps{}_p2_k2_batch{}.pdf'.format(N_hid, Nep, eps0, N_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    x_norm = x - np.mean(x, axis=1, keepdims=True)\n",
    "    x_norm = x_norm / np.linalg.norm(x_norm, ord=2, axis=1, keepdims=True)\n",
    "    return attach(x_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing: compute hidden neurons for train, val, and test data\n",
    "This will be the input to the second layer, which will be trained with SGD to do digit recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center and normalize input data to unit vectors  \n",
    "synapses = attach(synapses)\n",
    "x_train0 = normalize(x_train)\n",
    "x_val0 = normalize(x_val)\n",
    "x_test0 = normalize(x_test)\n",
    "\n",
    "x_hid_train = forward(x_train0, synapses, p, N_hid, N_train, training=False)\n",
    "x_hid_val = forward(x_val0, synapses, p, N_hid, N_val, training=False)\n",
    "x_hid_test = forward(x_test0, synapses, p, N_hid, N_test, training=False)\n",
    "\n",
    "n=4.5\n",
    "x_hid_train = (x_hid_train * (x_hid_train>0)) ** n\n",
    "x_hid_val = (x_hid_val * (x_hid_val>0)) ** n\n",
    "x_hid_test = (x_hid_test * (x_hid_test>0)) ** n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert everything to numpy tensors\n",
    "x_hid_train = detach(x_hid_train)\n",
    "x_hid_val = detach(x_hid_val)\n",
    "x_hid_test = detach(x_hid_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the BIO model\n",
    "- Input is the pre-computed hidden activations.\n",
    "- Output layer is a fully connected layer with `softmax activation`\n",
    "- Loss function is the `categorical cross-entropy`\n",
    "- Optimizer is `Adam` with initial learning rate of 0.1 which decays exponentially every `20` epochs with a decay rate of `0.5`\n",
    "- Batch size is `10` and number of epochs is `500`\n",
    "\n",
    "Set `verbose=1` in the `model.fit()` function to see the loss and accuracy values after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_model = keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(N_hid,)),\n",
    "    layers.Dense(Nc),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('softmax')\n",
    "])\n",
    "\n",
    "print(bio_model.summary())\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=4e-1, decay_steps=Nep*20, decay_rate=0.7, staircase=True)\n",
    "opt = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "bio_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "bio_logs = bio_model.fit(x_hid_train, y_train, validation_data=(x_hid_test, y_test), batch_size=100, epochs=Nep//2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the backprop model\n",
    "- One fully-connected hidden layer with the same number of neurons as the BIO model, i.e. `N_hid`\n",
    "    - Activation is 'relu'\n",
    "- Another fully-connected output layer with softmax activation\n",
    "- Same loss and optimizer as BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_model = keras.Sequential([\n",
    "    layers.Input(shape=(N_in,)),\n",
    "    layers.Dense(N_hid),\n",
    "    layers.Activation(\"relu\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(Nc, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "bp_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "bp_logs = bp_model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=1000, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot losses and accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_history = bio_logs.history\n",
    "bp_history = bp_logs.history\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(bio_history['loss'])\n",
    "plt.plot(bio_history['val_loss'])\n",
    "\n",
    "plt.plot(bp_history['loss'])\n",
    "plt.plot(bp_history['val_loss'])\n",
    "\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "# plt.ylim(0, 1)\n",
    "plt.xlim(0, 100)\n",
    "plt.legend(['Train_BIO', 'Validation_BIO', 'Train_BP', 'Validation_BP'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(bio_history['accuracy'])\n",
    "plt.plot(bio_history['val_accuracy'])\n",
    "\n",
    "plt.plot(bp_history['accuracy'])\n",
    "plt.plot(bp_history['val_accuracy'])\n",
    "\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim(0, 1)\n",
    "plt.xlim(0, 100)\n",
    "plt.legend(['Train_BIO', 'Validation_BIO', 'Train_BP', 'Validation_BP'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('BIO best validation accuracy:', max(bio_history['val_accuracy']))\n",
    "print('BP best validation accuracy:', max(bp_history['val_accuracy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot weights of the BP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# %matplotlib notebook\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(13, 5))\n",
    "ax[0].set_title('BP')\n",
    "ax[1].set_title('BIO')\n",
    "bp_weights = bp_model.layers[0].get_weights()[0].T\n",
    "draw_weights(bp_weights, Kx, Ky, ax[0])\n",
    "draw_weights(detach(synapses), Kx, Ky, ax[1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control Test: A linear model with the same architecture\n",
    "If a linear model with one hidden layer performs on paar with BIO, this means the images themselves are as representative as the biological hidden neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_model = keras.Sequential([\n",
    "    layers.Input(shape=(N_in)),\n",
    "    layers.Dense(N_hid),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(Nc, activation='softmax')\n",
    "])\n",
    "control_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "control_logs = control_model.fit(detach(x_train0), y_train, validation_data=(detach(x_val0), y_val), batch_size=1000, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Training Accuracy:\", control_logs.history['accuracy'][-1])\n",
    "print(\"Final Validation Accuracy:\", control_logs.history['val_accuracy'][-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
